{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf2f38d",
   "metadata": {},
   "source": [
    "# ChronoFit MVAA - Model Training Pipeline\n",
    "\n",
    "This notebook trains the AI model for personalized workout recommendations using:\n",
    "- **20,000 synthetic training records** with realistic parameter distributions\n",
    "- **User feedback data** weighted 2x for rapid adaptation\n",
    "- **RandomForest + MultiOutput Regression** for duration & intensity prediction\n",
    "- **Cross-validation** for robust performance evaluation\n",
    "- **Automatic model export** to .joblib files for Streamlit deployment\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. **Setup & Data Generation** - Create synthetic training data\n",
    "2. **Data Preprocessing** - Build feature transformation pipeline\n",
    "3. **Goal Classification** - Train goal classifier model\n",
    "4. **Model Training** - Train main regression model with GridSearch CV\n",
    "5. **Evaluation & Validation** - Assess model performance\n",
    "6. **User Feedback Integration** - Incorporate real user feedback\n",
    "7. **Model Export** - Save trained models for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d4c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274233e",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Training Data (20k records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daeb72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating 20000 synthetic training records...\n",
      "\n",
      "‚úÖ Generated 20000 records\n",
      "\n",
      "Feature statistics:\n",
      "            age  weight_kg  sleep_hrs   rhr_bpm  soreness  mental_stress  \\\n",
      "count  20000.00   20000.00   20000.00  20000.00  20000.00       20000.00   \n",
      "mean      35.90      77.83       6.86     62.47      2.20           2.71   \n",
      "std       11.91      13.81       1.09      6.32      1.17           1.22   \n",
      "min       18.00      45.00       4.00     45.00      1.00           1.00   \n",
      "25%       26.00      68.40       6.10     58.00      1.00           2.00   \n",
      "50%       33.00      77.80       7.00     62.00      2.00           3.00   \n",
      "75%       45.00      87.20       7.70     67.00      3.00           3.00   \n",
      "max       70.00     129.50      10.00     89.00      5.00           5.00   \n",
      "\n",
      "       calories_in  protein_g   carbs_g  nutrition_confidence  \n",
      "count     20000.00   20000.00  20000.00              20000.00  \n",
      "mean       2198.69      99.96    279.75                  3.01  \n",
      "std         397.36      24.95     60.24                  1.26  \n",
      "min         800.00      30.00     50.00                  1.00  \n",
      "25%        1929.00      83.20    239.30                  2.00  \n",
      "50%        2197.00     100.00    280.00                  3.00  \n",
      "75%        2463.25     116.90    320.32                  4.00  \n",
      "max        3791.00     187.30    506.90                  5.00  \n",
      "\n",
      "First 5 records:\n",
      "   age sex  weight_kg  sleep_hrs  rhr_bpm  soreness  mental_stress  \\\n",
      "0   40   M       65.6        7.6     61.0         2              1   \n",
      "1   33   M       69.2        5.8     74.0         1              4   \n",
      "2   28   M       60.3        6.9     60.0         5              3   \n",
      "3   20   M       76.8        8.0     62.0         2              5   \n",
      "4   30   M       88.4        7.9     60.0         1              2   \n",
      "\n",
      "   calories_in  protein_g  carbs_g  nutrition_confidence  \n",
      "0       2211.0       71.1    355.8                     3  \n",
      "1       2690.0      121.2    193.1                     2  \n",
      "2       1750.0       95.9    379.6                     1  \n",
      "3       2345.0       86.4    210.5                     3  \n",
      "4       2405.0       70.2    309.8                     1  \n"
     ]
    }
   ],
   "source": [
    "NUM_RECORDS = 20000\n",
    "print(f\"üîÑ Generating {NUM_RECORDS} synthetic training records...\\n\")\n",
    "\n",
    "# Realistic age distribution (bimodal: young athletes + older fitness enthusiasts)\n",
    "AGE = np.concatenate([\n",
    "    np.random.normal(28, 6, int(NUM_RECORDS * 0.6)).clip(18, 40).astype(int),\n",
    "    np.random.normal(48, 8, int(NUM_RECORDS * 0.4)).clip(40, 70).astype(int)\n",
    "])\n",
    "np.random.shuffle(AGE)\n",
    "AGE = AGE[:NUM_RECORDS]\n",
    "\n",
    "# Sex distribution\n",
    "SEX = np.random.choice(['M', 'F'], size=NUM_RECORDS, p=[0.6, 0.4])\n",
    "\n",
    "# Weight follows BMI-realistic distribution\n",
    "WEIGHT_KG = np.random.normal(78, 14, size=NUM_RECORDS).clip(45, 130).round(1)\n",
    "\n",
    "# Sleep: realistic with some sleep-deprived individuals\n",
    "SLEEP_HRS = np.concatenate([\n",
    "    np.random.normal(7.3, 0.8, int(NUM_RECORDS * 0.75)),\n",
    "    np.random.normal(5.5, 0.6, int(NUM_RECORDS * 0.25))\n",
    "]).clip(4.0, 10.0).round(1)\n",
    "np.random.shuffle(SLEEP_HRS)\n",
    "\n",
    "# RHR correlates with fitness/weight\n",
    "RHR_BASE = 62 + (WEIGHT_KG - 75) * 0.15\n",
    "RHR_BPM = (RHR_BASE + np.random.normal(0, 6, size=NUM_RECORDS)).clip(45, 95).round(0)\n",
    "\n",
    "# Soreness distribution (skewed toward low)\n",
    "SORENESS = np.random.choice([1, 2, 3, 4, 5], size=NUM_RECORDS, p=[0.35, 0.30, 0.20, 0.10, 0.05])\n",
    "\n",
    "# Mental stress\n",
    "MENTAL_STRESS = np.random.choice([1, 2, 3, 4, 5], size=NUM_RECORDS, p=[0.20, 0.25, 0.30, 0.15, 0.10])\n",
    "\n",
    "# Nutrition (user input estimates)\n",
    "CALORIES = np.random.normal(2200, 400, size=NUM_RECORDS).clip(800, 4000).round(0)\n",
    "PROTEIN_G = np.random.normal(100, 25, size=NUM_RECORDS).clip(30, 300).round(1)\n",
    "CARBS_G = np.random.normal(280, 60, size=NUM_RECORDS).clip(50, 600).round(1)\n",
    "\n",
    "# Confidence score in nutrition data\n",
    "NUTR_CONF_SCORE = np.random.choice([1, 2, 3, 4, 5], size=NUM_RECORDS, p=[0.15, 0.20, 0.30, 0.20, 0.15])\n",
    "\n",
    "# Create features dataframe\n",
    "X = pd.DataFrame({\n",
    "    'age': AGE,\n",
    "    'sex': SEX,\n",
    "    'weight_kg': WEIGHT_KG,\n",
    "    'sleep_hrs': SLEEP_HRS,\n",
    "    'rhr_bpm': RHR_BPM,\n",
    "    'soreness': SORENESS,\n",
    "    'mental_stress': MENTAL_STRESS,\n",
    "    'calories_in': CALORIES,\n",
    "    'protein_g': PROTEIN_G,\n",
    "    'carbs_g': CARBS_G,\n",
    "    'nutrition_confidence': NUTR_CONF_SCORE\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Generated {len(X)} records\")\n",
    "print(f\"\\nFeature statistics:\\n{X.describe().round(2)}\")\n",
    "print(f\"\\nFirst 5 records:\\n{X.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca307b21",
   "metadata": {},
   "source": [
    "## Step 2: Generate Target Variables (Duration & Intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5ad3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated target variables\n",
      "\n",
      "Goal distribution:\n",
      "goal\n",
      "Strength       7095\n",
      "Endurance      5926\n",
      "Maintenance    4019\n",
      "Flexibility    2960\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duration stats:\n",
      "count    20000.0\n",
      "mean        43.1\n",
      "std         12.4\n",
      "min         20.0\n",
      "25%         34.0\n",
      "50%         43.0\n",
      "75%         52.0\n",
      "max         82.0\n",
      "Name: duration_min, dtype: float64\n",
      "\n",
      "Intensity stats:\n",
      "count    20000.00\n",
      "mean         5.31\n",
      "std          2.08\n",
      "min          1.00\n",
      "25%          3.80\n",
      "50%          5.40\n",
      "75%          6.90\n",
      "max         10.00\n",
      "Name: intensity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Goal categories\n",
    "GOAL_LABELS = ['Strength', 'Endurance', 'Maintenance', 'Flexibility']\n",
    "GOALS = np.random.choice(GOAL_LABELS, size=NUM_RECORDS, p=[0.35, 0.30, 0.20, 0.15])\n",
    "\n",
    "# Base duration & intensity by goal\n",
    "goal_params = {\n",
    "    'Strength': (45, 8),      # (duration_min, intensity)\n",
    "    'Endurance': (60, 6),\n",
    "    'Maintenance': (40, 5),\n",
    "    'Flexibility': (30, 3)\n",
    "}\n",
    "\n",
    "DURATION_BASE = np.array([goal_params[g][0] for g in GOALS], dtype=float)\n",
    "INTENSITY_BASE = np.array([goal_params[g][1] for g in GOALS], dtype=float)\n",
    "\n",
    "# Modifiers based on readiness metrics\n",
    "sleep_modifier = (SLEEP_HRS - 5.5) * 2  # More sleep = more intense/longer\n",
    "rhr_modifier = (RHR_BPM - 50) / 10      # Lower RHR (fitter) = higher intensity\n",
    "soreness_modifier = (6 - SORENESS) * 1.5  # Less sore = higher intensity\n",
    "stress_modifier = (6 - MENTAL_STRESS) * 1.2  # Less stressed = higher intensity\n",
    "\n",
    "# Apply modifiers\n",
    "DURATION = (DURATION_BASE + sleep_modifier * 3 - soreness_modifier * 2).clip(20, 120).round(0)\n",
    "INTENSITY = (INTENSITY_BASE + sleep_modifier * 0.5 + rhr_modifier * 0.3 - soreness_modifier * 0.3 - stress_modifier * 0.2).clip(1, 10).round(1)\n",
    "\n",
    "# Create target dataframe\n",
    "Y = pd.DataFrame({\n",
    "    'goal': GOALS,\n",
    "    'duration_min': DURATION,\n",
    "    'intensity': INTENSITY\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Generated target variables\")\n",
    "print(f\"\\nGoal distribution:\\n{Y['goal'].value_counts()}\")\n",
    "print(f\"\\nDuration stats:\\n{Y['duration_min'].describe().round(2)}\")\n",
    "print(f\"\\nIntensity stats:\\n{Y['intensity'].describe().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f4bd5",
   "metadata": {},
   "source": [
    "## Step 3: Build Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1ca944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing pipeline built\n",
      "Processed features shape: (20000, 11)\n",
      "Feature names after transformation: 11 features\n"
     ]
    }
   ],
   "source": [
    "# Define feature types\n",
    "categorical_features = ['sex']\n",
    "numerical_features = ['age', 'weight_kg', 'sleep_hrs', 'rhr_bpm', 'soreness', 'mental_stress', \n",
    "                      'calories_in', 'protein_g', 'carbs_g', 'nutrition_confidence']\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)\n",
    "])\n",
    "\n",
    "# Fit and transform features\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"‚úÖ Preprocessing pipeline built\")\n",
    "print(f\"Processed features shape: {X_processed.shape}\")\n",
    "print(f\"Feature names after transformation: {len(X_processed[0])} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6099c4a",
   "metadata": {},
   "source": [
    "## Step 4: Train-Test Split with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b860f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train-Test Split Complete\n",
      "Training set: 16000 samples (80.0%)\n",
      "Test set: 4000 samples (20.0%)\n",
      "\n",
      "Goal distribution in train set:\n",
      "goal\n",
      "Strength       5676\n",
      "Endurance      4741\n",
      "Maintenance    3215\n",
      "Flexibility    2368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Goal distribution in test set:\n",
      "goal\n",
      "Strength       1419\n",
      "Endurance      1185\n",
      "Maintenance     804\n",
      "Flexibility     592\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified split on goals for balanced distribution\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in sss.split(X_processed, Y['goal']):\n",
    "    X_train, X_test = X_processed[train_idx], X_processed[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "print(f\"‚úÖ Train-Test Split Complete\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nGoal distribution in train set:\\n{Y_train['goal'].value_counts()}\")\n",
    "print(f\"\\nGoal distribution in test set:\\n{Y_test['goal'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814aafce",
   "metadata": {},
   "source": [
    "## Step 5: Train Goal Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b047d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Goal Classification Model Trained\n",
      "Training accuracy: 0.7480\n",
      "Test accuracy: 0.3483\n",
      "\n",
      "Goal classes: ['Endurance' 'Flexibility' 'Maintenance' 'Strength']\n"
     ]
    }
   ],
   "source": [
    "# Encode goal labels\n",
    "goal_encoder = LabelEncoder()\n",
    "Y_goal_encoded = goal_encoder.fit_transform(Y_train['goal'])\n",
    "Y_goal_test_encoded = goal_encoder.transform(Y_test['goal'])\n",
    "\n",
    "# Train goal classifier\n",
    "goal_classifier = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "goal_classifier.fit(X_train, Y_goal_encoded)\n",
    "\n",
    "# Evaluate goal classifier\n",
    "Y_goal_pred_train = goal_classifier.predict(X_train)\n",
    "Y_goal_pred_test = goal_classifier.predict(X_test)\n",
    "\n",
    "goal_accuracy_train = (Y_goal_pred_train == Y_goal_encoded).mean()\n",
    "goal_accuracy_test = (Y_goal_pred_test == Y_goal_test_encoded).mean()\n",
    "\n",
    "print(\"‚úÖ Goal Classification Model Trained\")\n",
    "print(f\"Training accuracy: {goal_accuracy_train:.4f}\")\n",
    "print(f\"Test accuracy: {goal_accuracy_test:.4f}\")\n",
    "print(f\"\\nGoal classes: {goal_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fde93c",
   "metadata": {},
   "source": [
    "## Step 6: Train Main Regression Model with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ceaaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running GridSearch CV (this may take 2-3 minutes)...\n",
      "\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "‚úÖ GridSearch Complete in 231.1s\n",
      "Best parameters: {'estimator__max_depth': 12, 'estimator__min_samples_leaf': 4, 'estimator__min_samples_split': 10, 'estimator__n_estimators': 200}\n",
      "Best CV score: 0.3183\n",
      "\n",
      "‚úÖ GridSearch Complete in 231.1s\n",
      "Best parameters: {'estimator__max_depth': 12, 'estimator__min_samples_leaf': 4, 'estimator__min_samples_split': 10, 'estimator__n_estimators': 200}\n",
      "Best CV score: 0.3183\n"
     ]
    }
   ],
   "source": [
    "# Prepare targets for regression (duration + intensity)\n",
    "Y_values = Y_train[['duration_min', 'intensity']].values\n",
    "\n",
    "# Define parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200],\n",
    "    'estimator__max_depth': [12, 15, 18],\n",
    "    'estimator__min_samples_split': [5, 10],\n",
    "    'estimator__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Base regressor\n",
    "base_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# MultiOutput wrapper for predicting both duration and intensity\n",
    "multi_model = MultiOutputRegressor(base_rf)\n",
    "\n",
    "# GridSearch for best parameters\n",
    "print(\"üîç Running GridSearch CV (this may take 2-3 minutes)...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(multi_model, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, Y_values)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "best_mvva_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\n‚úÖ GridSearch Complete in {elapsed:.1f}s\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fb2d0",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4ccfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "  Duration - RMSE: 7.96 min, MAE: 6.43 min, R¬≤: 0.5878\n",
      "  Intensity - RMSE: 1.32, MAE: 1.08, R¬≤: 0.5944\n",
      "\n",
      "Test Set Metrics:\n",
      "  Duration - RMSE: 10.13 min, MAE: 8.22 min, R¬≤: 0.3308\n",
      "  Intensity - RMSE: 1.72, MAE: 1.41, R¬≤: 0.3358\n",
      "\n",
      "5-Fold Cross-Validation R¬≤ Scores: [0.31318104 0.31592473 0.33447834 0.32253356 0.31745378]\n",
      "Mean CV R¬≤ Score: 0.3207 (+/- 0.0075)\n",
      "\n",
      "‚úÖ Model evaluation complete\n",
      "\n",
      "5-Fold Cross-Validation R¬≤ Scores: [0.31318104 0.31592473 0.33447834 0.32253356 0.31745378]\n",
      "Mean CV R¬≤ Score: 0.3207 (+/- 0.0075)\n",
      "\n",
      "‚úÖ Model evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on train and test sets\n",
    "Y_train_pred = best_mvva_model.predict(X_train)\n",
    "Y_test_pred = best_mvva_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(Y_true, Y_pred, set_name):\n",
    "    \"\"\"Calculate RMSE, MAE, and R2 for both targets\"\"\"\n",
    "    rmse_duration = np.sqrt(mean_squared_error(Y_true[:, 0], Y_pred[:, 0]))\n",
    "    rmse_intensity = np.sqrt(mean_squared_error(Y_true[:, 1], Y_pred[:, 1]))\n",
    "    mae_duration = mean_absolute_error(Y_true[:, 0], Y_pred[:, 0])\n",
    "    mae_intensity = mean_absolute_error(Y_true[:, 1], Y_pred[:, 1])\n",
    "    r2_duration = r2_score(Y_true[:, 0], Y_pred[:, 0])\n",
    "    r2_intensity = r2_score(Y_true[:, 1], Y_pred[:, 1])\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"  Duration - RMSE: {rmse_duration:.2f} min, MAE: {mae_duration:.2f} min, R¬≤: {r2_duration:.4f}\")\n",
    "    print(f\"  Intensity - RMSE: {rmse_intensity:.2f}, MAE: {mae_intensity:.2f}, R¬≤: {r2_intensity:.4f}\")\n",
    "    \n",
    "    return rmse_duration, rmse_intensity, mae_duration, mae_intensity, r2_duration, r2_intensity\n",
    "\n",
    "# Evaluate on training set (use only numeric columns: duration_min and intensity)\n",
    "Y_train_numeric = Y_train[['duration_min', 'intensity']].values\n",
    "rmse_dur_train, rmse_int_train, mae_dur_train, mae_int_train, r2_dur_train, r2_int_train = \\\n",
    "    calculate_metrics(Y_train_numeric, Y_train_pred, \"Training Set\")\n",
    "\n",
    "# Evaluate on test set (use only numeric columns: duration_min and intensity)\n",
    "Y_test_numeric = Y_test[['duration_min', 'intensity']].values\n",
    "rmse_dur_test, rmse_int_test, mae_dur_test, mae_int_test, r2_dur_test, r2_int_test = \\\n",
    "    calculate_metrics(Y_test_numeric, Y_test_pred, \"Test Set\")\n",
    "\n",
    "# Cross-validation scores (use only numeric targets)\n",
    "cv_scores = cross_val_score(best_mvva_model, X_train, Y_train_numeric, cv=5, scoring='r2')\n",
    "print(f\"\\n5-Fold Cross-Validation R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Mean CV R¬≤ Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492db8c",
   "metadata": {},
   "source": [
    "## Step 8: Integrate User Feedback Data (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb5ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading feedback from feedback_history.csv...\n",
      "‚úÖ Loaded 4 feedback records\n",
      "\n",
      "Feedback Summary:\n",
      "  Columns: ['timestamp', 'age', 'sex', 'weight_kg', 'sleep_hrs', 'rhr_bpm', 'soreness_before', 'mental_stress', 'calories_in', 'protein_g', 'carbs_g', 'predicted_goal', 'recommended_duration', 'recommended_intensity', 'workout_completion_pct', 'actual_intensity', 'difficulty_feedback', 'recovery_feeling', 'soreness_next_day_expected', 'would_repeat']\n",
      "  Date range: 2025-11-13 10:05:41.293369 to 2025-11-13 10:39:30.525258\n",
      "\n",
      "üí° In production:\n",
      "  - User feedback is saved to MongoDB\n",
      "  - Background thread retrains model every 3 feedbacks\n",
      "  - See chronofit_app.py for continuous learning implementation\n"
     ]
    }
   ],
   "source": [
    "# Check for feedback from MongoDB or local CSV\n",
    "feedback_file = 'feedback_history.csv'\n",
    "feedback_data = []\n",
    "\n",
    "if os.path.exists(feedback_file):\n",
    "    print(f\"üìä Loading feedback from {feedback_file}...\")\n",
    "    feedback_df = pd.read_csv(feedback_file)\n",
    "    print(f\"‚úÖ Loaded {len(feedback_df)} feedback records\")\n",
    "    \n",
    "    # Show feedback summary\n",
    "    if len(feedback_df) > 0:\n",
    "        print(f\"\\nFeedback Summary:\")\n",
    "        print(f\"  Columns: {list(feedback_df.columns)}\")\n",
    "        print(f\"  Date range: {feedback_df['timestamp'].min()} to {feedback_df['timestamp'].max()}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No feedback file found. Using only synthetic data for training.\")\n",
    "    feedback_df = pd.DataFrame()\n",
    "\n",
    "# Note: Feedback retraining will happen automatically in chronofit_app.py when 3+ feedbacks are submitted\n",
    "print(\"\\nüí° In production:\")\n",
    "print(\"  - User feedback is saved to MongoDB\")\n",
    "print(\"  - Background thread retrains model every 3 feedbacks\")\n",
    "print(\"  - See chronofit_app.py for continuous learning implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf9696",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57c3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature Importance Analysis:\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "             feature  duration_importance  intensity_importance  avg_importance\n",
      "           sleep_hrs             0.468712              0.451929        0.460321\n",
      "            soreness             0.136000              0.094135        0.115068\n",
      "           protein_g             0.065936              0.069202        0.067569\n",
      "         calories_in             0.067099              0.067851        0.067475\n",
      "             carbs_g             0.065153              0.068575        0.066864\n",
      "           weight_kg             0.062678              0.068225        0.065452\n",
      "             rhr_bpm             0.043576              0.058886        0.051231\n",
      "                 age             0.046917              0.052045        0.049481\n",
      "       mental_stress             0.017806              0.041217        0.029512\n",
      "nutrition_confidence             0.018813              0.020193        0.019503\n",
      "\n",
      "Duration Prediction - Top 5 Features:\n",
      "  sleep_hrs: 0.4687\n",
      "  soreness: 0.1360\n",
      "  calories_in: 0.0671\n",
      "  protein_g: 0.0659\n",
      "  carbs_g: 0.0652\n",
      "\n",
      "Intensity Prediction - Top 5 Features:\n",
      "  sleep_hrs: 0.4519\n",
      "  soreness: 0.0941\n",
      "  protein_g: 0.0692\n",
      "  carbs_g: 0.0686\n",
      "  weight_kg: 0.0682\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the model\n",
    "feature_importance_duration = best_mvva_model.estimators_[0].feature_importances_\n",
    "feature_importance_intensity = best_mvva_model.estimators_[1].feature_importances_\n",
    "\n",
    "# Get feature names (OneHotEncoded sex + numerical features)\n",
    "feature_names = ['sex_M'] + numerical_features\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'duration_importance': feature_importance_duration,\n",
    "    'intensity_importance': feature_importance_intensity\n",
    "})\n",
    "importance_df['avg_importance'] = (importance_df['duration_importance'] + importance_df['intensity_importance']) / 2\n",
    "importance_df = importance_df.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "print(\"‚úÖ Feature Importance Analysis:\")\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nDuration Prediction - Top 5 Features:\")\n",
    "for i, row in importance_df.nlargest(5, 'duration_importance').iterrows():\n",
    "    print(f\"  {row['feature']}: {row['duration_importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nIntensity Prediction - Top 5 Features:\")\n",
    "for i, row in importance_df.nlargest(5, 'intensity_importance').iterrows():\n",
    "    print(f\"  {row['feature']}: {row['intensity_importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41382f",
   "metadata": {},
   "source": [
    "## Step 10: Save Models for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5529cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving trained models...\n",
      "\n",
      "‚úÖ Saved main model: mvva_model_v2.joblib\n",
      "‚úÖ Saved preprocessor: mvva_preprocessor_v2.joblib\n",
      "‚úÖ Saved main model: mvva_model_v2.joblib\n",
      "‚úÖ Saved preprocessor: mvva_preprocessor_v2.joblib\n",
      "‚úÖ Saved goal classifier: mvva_goal_classifier_v2.joblib\n",
      "‚úÖ Saved goal encoder: mvva_goal_encoder_v2.joblib\n",
      "\n",
      "üì¶ Total model size: 69.3 MB\n",
      "\n",
      "‚úÖ All models saved successfully!\n",
      "\n",
      "üìù Models are ready for deployment:\n",
      "  - Streamlit app: chronofit_app.py\n",
      "  - MongoDB handler: mongodb_handler.py\n",
      "  - Deploy to: https://share.streamlit.io\n",
      "‚úÖ Saved goal classifier: mvva_goal_classifier_v2.joblib\n",
      "‚úÖ Saved goal encoder: mvva_goal_encoder_v2.joblib\n",
      "\n",
      "üì¶ Total model size: 69.3 MB\n",
      "\n",
      "‚úÖ All models saved successfully!\n",
      "\n",
      "üìù Models are ready for deployment:\n",
      "  - Streamlit app: chronofit_app.py\n",
      "  - MongoDB handler: mongodb_handler.py\n",
      "  - Deploy to: https://share.streamlit.io\n"
     ]
    }
   ],
   "source": [
    "# Define model file paths\n",
    "MODEL_PATH = 'mvva_model_v2.joblib'\n",
    "PREPROCESSOR_PATH = 'mvva_preprocessor_v2.joblib'\n",
    "GOAL_CLASSIFIER_PATH = 'mvva_goal_classifier_v2.joblib'\n",
    "GOAL_ENCODER_PATH = 'mvva_goal_encoder_v2.joblib'\n",
    "\n",
    "# Save all models\n",
    "print(\"üíæ Saving trained models...\\n\")\n",
    "\n",
    "joblib.dump(best_mvva_model, MODEL_PATH)\n",
    "print(f\"‚úÖ Saved main model: {MODEL_PATH}\")\n",
    "\n",
    "joblib.dump(preprocessor, PREPROCESSOR_PATH)\n",
    "print(f\"‚úÖ Saved preprocessor: {PREPROCESSOR_PATH}\")\n",
    "\n",
    "joblib.dump(goal_classifier, GOAL_CLASSIFIER_PATH)\n",
    "print(f\"‚úÖ Saved goal classifier: {GOAL_CLASSIFIER_PATH}\")\n",
    "\n",
    "joblib.dump(goal_encoder, GOAL_ENCODER_PATH)\n",
    "print(f\"‚úÖ Saved goal encoder: {GOAL_ENCODER_PATH}\")\n",
    "\n",
    "# Get file sizes\n",
    "import os\n",
    "total_size = sum(os.path.getsize(f) for f in [MODEL_PATH, PREPROCESSOR_PATH, GOAL_CLASSIFIER_PATH, GOAL_ENCODER_PATH])\n",
    "print(f\"\\nüì¶ Total model size: {total_size / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\n‚úÖ All models saved successfully!\")\n",
    "print(\"\\nüìù Models are ready for deployment:\")\n",
    "print(f\"  - Streamlit app: chronofit_app.py\")\n",
    "print(f\"  - MongoDB handler: mongodb_handler.py\")\n",
    "print(f\"  - Deploy to: https://share.streamlit.io\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
